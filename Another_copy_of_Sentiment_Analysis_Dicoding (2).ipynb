{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Proyek Analisis Sentimen pada Google Reviews\n",
        "- **Nama:** Yosriko Rahmat Karoni Sabelekake\n",
        "- **Email:** yosrikosabelekake@gmail.com\n",
        "- **ID Dicoding:** yosriko\n"
      ],
      "metadata": {
        "id": "qVCpm9ihUIfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import modul yang diperlukan"
      ],
      "metadata": {
        "id": "vNpYekOkgYgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac1Si33an1K8",
        "outputId": "f5f686a3-b607-46f1-87d0-a42f0a9a7062"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.2)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYw8nK3UvrFd",
        "outputId": "ffefe8b3-7303-4ece-e851-ba2389996a98"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m204.8/209.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAki7dElD-DK",
        "outputId": "2ce25cc7-5601-4fca-8ecd-ab6a2fb7dc16"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from gensim.models import Word2Vec\n",
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib"
      ],
      "metadata": {
        "id": "W8AWV7c3gXcE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raNSB9aLunSe",
        "outputId": "7a504b0e-d3dd-47d8-f809-a34e7def589c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Sentiment Labeling"
      ],
      "metadata": {
        "id": "3z33OZB7VWX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/dataset/playstore_reviews.csv')\n",
        "data = data[['Review']]"
      ],
      "metadata": {
        "id": "bn_RtXRwUHdI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model and the TF-IDF vectorizer\n",
        "svm_model = joblib.load('/content/svm_model_scenario_1.pkl')\n",
        "tfidf = joblib.load('/content/tfidf_vectorizer_scenario_1.pkl')\n",
        "\n",
        "# Function to predict sentiment of custom sentences\n",
        "def predict_custom_sentences(custom_sentences):\n",
        "    # Replace None, NaN, or empty strings with an empty string\n",
        "    custom_sentences = [sentence if isinstance(sentence, str) else \"\" for sentence in custom_sentences]\n",
        "\n",
        "    # Transform custom sentences using the loaded TF-IDF vectorizer\n",
        "    X_custom = tfidf.transform(custom_sentences)\n",
        "\n",
        "    # Predict using the loaded SVM model\n",
        "    predictions = svm_model.predict(X_custom)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Applying the predictions to the DataFrame\n",
        "def label_sentiment(text):\n",
        "    # Use the predict_custom_sentences function to get the sentiment\n",
        "    prediction = predict_custom_sentences([text])[0]  # Get the first prediction for a single text\n",
        "    return prediction\n",
        "\n",
        "# Applying the function to the 'Text' column in your DataFrame\n",
        "data['Sentiment'] = data['Review'].apply(label_sentiment)\n"
      ],
      "metadata": {
        "id": "Em5Z7QW2odC7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.head())\n",
        "print(len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I73GERAoiYE",
        "outputId": "0bb985a1-402a-407f-e794-117a839c7a74"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Review Sentiment\n",
            "0  Konsep yang keren menggabungkan 2d dengan 3d d...   Positif\n",
            "1  Lumayan sulit buat ku karena ada tikus tanah y...   Positif\n",
            "2  Gamenya bakalan sangat boring di awal2, karna ...   Positif\n",
            "3  Untuk update selanjutnya tolong tambahkan bebe...   Positif\n",
            "4  Ini game pembuat nya gak mau buat player senen...   Positif\n",
            "45588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing Data\n"
      ],
      "metadata": {
        "id": "6XgGQg8qslJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Case Folding: convert text to lowercase\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Removal Special Characters and Digits: remove special characters, digits, and punctuations\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Stopword Removal (using NLTK and Sastrawi)\n",
        "    nltk_stopwords = set(stopwords.words('indonesian'))\n",
        "    filtered_tokens = [word for word in tokens if word not in nltk_stopwords]\n",
        "\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "data['Cleaned_Review'] = data['Review'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "p6q4QWvzpHBb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URt-nVIYP8dA",
        "outputId": "1587a5db-2e20-45d4-f569-eb40eac35e49"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 45588 entries, 0 to 45587\n",
            "Data columns (total 3 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   Review          45587 non-null  object\n",
            " 1   Sentiment       45588 non-null  object\n",
            " 2   Cleaned_Review  45588 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 1.0+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario 1: SVM, TF-IDF, 80/20 Split\n"
      ],
      "metadata": {
        "id": "OfCmO2J2xy8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scenario_1(data):\n",
        "    # TF-IDF\n",
        "    tfidf = TfidfVectorizer(max_features=1000)\n",
        "    X = tfidf.fit_transform(data['Cleaned_Review'])\n",
        "    y = data['Sentiment']\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train SVM\n",
        "    svm_model = SVC()\n",
        "    svm_model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluation\n",
        "    y_pred = svm_model.predict(X_test)\n",
        "    print(\"Scenario 1 - SVM, TF-IDF, 80/20 Split:\")\n",
        "    print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yitjGXZIs5c_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario 2: RF, Word2Vec, 80/20 Split"
      ],
      "metadata": {
        "id": "f3WJz_TQx2bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scenario_2(data):\n",
        "    # Word2Vec\n",
        "    tokenized_reviews = [review.split() for review in data['Cleaned_Review']]\n",
        "    w2v_model = Word2Vec(sentences=tokenized_reviews, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "    # Transform reviews into average Word2Vec vectors\n",
        "    def get_avg_word2vec(tokens):\n",
        "        vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n",
        "        if len(vectors) == 0:\n",
        "            return np.zeros(100)\n",
        "        return np.mean(vectors, axis=0)\n",
        "\n",
        "    X = np.array([get_avg_word2vec(tokens) for tokens in tokenized_reviews])\n",
        "    y = data['Sentiment']\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train Random Forest\n",
        "    rf_model = RandomForestClassifier()\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluation\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "    print(\"\\nScenario 2 - RF, Word2Vec, 80/20 Split:\")\n",
        "    print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "QWyd-IgKx74G"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario 3: RF, TF-IDF, 70/30 Split\n"
      ],
      "metadata": {
        "id": "L5bTTHvVx-Xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scenario_3(data):\n",
        "    # TF-IDF\n",
        "    tfidf = TfidfVectorizer(max_features=1000)\n",
        "    X = tfidf.fit_transform(data['Cleaned_Review'])\n",
        "    y = data['Sentiment']\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Train Random Forest\n",
        "    rf_model = RandomForestClassifier()\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluation\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "    print(\"\\nScenario 3 - RF, TF-IDF, 70/30 Split:\")\n",
        "    print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "a_5kXozJx-ir"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running all Scenarios"
      ],
      "metadata": {
        "id": "Nrf3DVu4s4sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scenario_1(data)"
      ],
      "metadata": {
        "id": "7h6XEZz8yGWC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d6e2d43-bef3-4909-fc9c-5e77fdd9cda1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scenario 1 - SVM, TF-IDF, 80/20 Split:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Negatif       0.90      0.90      0.90       968\n",
            "      Netral       0.86      0.77      0.81       411\n",
            "     Positif       0.98      0.99      0.99      7739\n",
            "\n",
            "    accuracy                           0.97      9118\n",
            "   macro avg       0.91      0.89      0.90      9118\n",
            "weighted avg       0.97      0.97      0.97      9118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scenario_2(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_vL3JHtQBrN",
        "outputId": "767f963b-3e09-43ef-8b63-6758901a8f40"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scenario 2 - RF, Word2Vec, 80/20 Split:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Negatif       0.75      0.69      0.72       968\n",
            "      Netral       0.66      0.40      0.50       411\n",
            "     Positif       0.94      0.97      0.96      7739\n",
            "\n",
            "    accuracy                           0.92      9118\n",
            "   macro avg       0.79      0.69      0.72      9118\n",
            "weighted avg       0.91      0.92      0.91      9118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scenario_3(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_KOUtxcQB71",
        "outputId": "68c7b174-fd73-4984-dffa-e8503048dd54"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scenario 3 - RF, TF-IDF, 70/30 Split:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Negatif       0.74      0.82      0.78      1484\n",
            "      Netral       0.75      0.48      0.59       703\n",
            "     Positif       0.97      0.97      0.97     11490\n",
            "\n",
            "    accuracy                           0.93     13677\n",
            "   macro avg       0.82      0.76      0.78     13677\n",
            "weighted avg       0.93      0.93      0.93     13677\n",
            "\n"
          ]
        }
      ]
    }
  ]
}